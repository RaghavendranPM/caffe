// Copyright 2014 BVLC and contributors.

#include <cstring>
#include <vector>

#include "cuda_runtime.h"
#include "gtest/gtest.h"
#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/filler.hpp"
#include "caffe/vision_layers.hpp"
#include "caffe/test/test_gradient_check_util.hpp"

#include "caffe/test/test_caffe_main.hpp"

namespace caffe {

template <typename Dtype>
class SPPDetectorLayerTest : public ::testing::Test {
 protected:
  SPPDetectorLayerTest()
      : blob_bottom_data_(new Blob<Dtype>()),
        blob_bottom_conv5_windows_(new Blob<Dtype>()),
        blob_bottom_conv5_scales_(new Blob<Dtype>()),
        blob_top_(new Blob<Dtype>()) {}
  virtual void SetUp() {
    blob_bottom_vec_.push_back(blob_bottom_data_);
    blob_bottom_vec_.push_back(blob_bottom_conv5_windows_);
    blob_bottom_vec_.push_back(blob_bottom_conv5_scales_);
    blob_top_vec_.push_back(blob_top_);
  }
  virtual ~SPPDetectorLayerTest() {
    delete blob_bottom_data_;
    delete blob_bottom_conv5_windows_;
    delete blob_bottom_conv5_scales_;
    delete blob_top_;
  }
  Blob<Dtype>* const blob_bottom_data_;
  Blob<Dtype>* const blob_bottom_conv5_windows_;
  Blob<Dtype>* const blob_bottom_conv5_scales_;
  Blob<Dtype>* const blob_top_;
  vector<Blob<Dtype>*> blob_bottom_vec_;
  vector<Blob<Dtype>*> blob_top_vec_;

  void TestForward() {
    const int width = 6;
    const int height = 6;
    const int channels = 1;
    const int scale_num = 5;
    const int proposal_num = 10;
    blob_bottom_data_->Reshape(scale_num, channels, height, width);
    blob_bottom_conv5_windows_->Reshape(1, 1, proposal_num, 4);
    blob_bottom_conv5_scales_->Reshape(1, 1, 1, proposal_num);

    // Input: 5x 1 channels of:
    // [35     1     6    26    19    24]
    // [ 3    32     7    21    23    25]
    // [31     9     2    22    27    20]
    // [ 8    28    33    17    10    15] + scale * eye(6, 6)
    // [30     5    34    12    14    16]
    // [ 4    36    29    13    18    11]
    // (this is generated by magic(6) in MATLAB)
    for (int i = 0; i < 36 * channels * scale_num; i += 36) {
      blob_bottom_data_->mutable_cpu_data()[i +  0] = (i / 36) + 35;
      blob_bottom_data_->mutable_cpu_data()[i +  1] = (i / 36) + 1;
      blob_bottom_data_->mutable_cpu_data()[i +  2] = (i / 36) + 6;
      blob_bottom_data_->mutable_cpu_data()[i +  3] = (i / 36) + 26;
      blob_bottom_data_->mutable_cpu_data()[i +  4] = (i / 36) + 19;
      blob_bottom_data_->mutable_cpu_data()[i +  5] = (i / 36) + 24;
      blob_bottom_data_->mutable_cpu_data()[i +  6] = (i / 36) + 3;
      blob_bottom_data_->mutable_cpu_data()[i +  7] = (i / 36) + 32;
      blob_bottom_data_->mutable_cpu_data()[i +  8] = (i / 36) + 7;
      blob_bottom_data_->mutable_cpu_data()[i +  9] = (i / 36) + 21;
      blob_bottom_data_->mutable_cpu_data()[i + 10] = (i / 36) + 23;
      blob_bottom_data_->mutable_cpu_data()[i + 11] = (i / 36) + 25;
      blob_bottom_data_->mutable_cpu_data()[i + 12] = (i / 36) + 31;
      blob_bottom_data_->mutable_cpu_data()[i + 13] = (i / 36) + 9;
      blob_bottom_data_->mutable_cpu_data()[i + 14] = (i / 36) + 2;
      blob_bottom_data_->mutable_cpu_data()[i + 15] = (i / 36) + 22;
      blob_bottom_data_->mutable_cpu_data()[i + 16] = (i / 36) + 27;
      blob_bottom_data_->mutable_cpu_data()[i + 17] = (i / 36) + 20;
      blob_bottom_data_->mutable_cpu_data()[i + 18] = (i / 36) + 8;
      blob_bottom_data_->mutable_cpu_data()[i + 19] = (i / 36) + 28;
      blob_bottom_data_->mutable_cpu_data()[i + 20] = (i / 36) + 33;
      blob_bottom_data_->mutable_cpu_data()[i + 21] = (i / 36) + 17;
      blob_bottom_data_->mutable_cpu_data()[i + 22] = (i / 36) + 10;
      blob_bottom_data_->mutable_cpu_data()[i + 23] = (i / 36) + 15;
      blob_bottom_data_->mutable_cpu_data()[i + 24] = (i / 36) + 30;
      blob_bottom_data_->mutable_cpu_data()[i + 25] = (i / 36) + 5;
      blob_bottom_data_->mutable_cpu_data()[i + 26] = (i / 36) + 34;
      blob_bottom_data_->mutable_cpu_data()[i + 27] = (i / 36) + 12;
      blob_bottom_data_->mutable_cpu_data()[i + 28] = (i / 36) + 14;
      blob_bottom_data_->mutable_cpu_data()[i + 29] = (i / 36) + 16;
      blob_bottom_data_->mutable_cpu_data()[i + 30] = (i / 36) + 4;
      blob_bottom_data_->mutable_cpu_data()[i + 31] = (i / 36) + 36;
      blob_bottom_data_->mutable_cpu_data()[i + 32] = (i / 36) + 29;
      blob_bottom_data_->mutable_cpu_data()[i + 33] = (i / 36) + 13;
      blob_bottom_data_->mutable_cpu_data()[i + 34] = (i / 36) + 18;
      blob_bottom_data_->mutable_cpu_data()[i + 35] = (i / 36) + 11;
    }
    // 3 valid windows, and the rest are zeros
    Dtype conv5_windows[40] = {0, 1, 6, 4, 3, 0, 6, 5, 2, 4, 3, 6};
    for (int i = 0; i < 40; i++) {
      blob_bottom_conv5_windows_->mutable_cpu_data()[i] = conv5_windows[i];
    }
    // 3 valid scales, and the rest are zeros
    Dtype conv5_scales[10] = {0, 4, 2};
    for (int i = 0; i < 10; i++) {
      blob_bottom_conv5_scales_->mutable_cpu_data()[i] = conv5_scales[i];
    }

    LayerParameter layer_param;
    SpatialPyramidPoolingParameter* spatial_pyramid_pooling_param =
        layer_param.mutable_spatial_pyramid_pooling_param();
    spatial_pyramid_pooling_param->set_pool(
        SpatialPyramidPoolingParameter_PoolMethod_MAX);
    spatial_pyramid_pooling_param->add_spatial_bin(3);
    spatial_pyramid_pooling_param->add_spatial_bin(2);
    SPPDetectorLayer<Dtype> layer(layer_param);
    layer.SetUp(blob_bottom_vec_, &blob_top_vec_);
    EXPECT_EQ(blob_top_->num(), proposal_num);
    EXPECT_EQ(blob_top_->channels(), 1);
    EXPECT_EQ(blob_top_->height(), 1);
    EXPECT_EQ(blob_top_->width(), 3*3+2*2);
    layer.Forward(blob_bottom_vec_, &blob_top_vec_);
   
    // Expected output 10 * 1 * 1 * (3*3+2*2)
    // [ 32    7   26   28   33   22   36   34   13   32   26   36   34 ]
    // [ 32   37   21   34   38   18   40   40   22   38   38   40   38 ]
    // [ 29   29   22   29   29   22   29   29   22   29   22   29   22 ]
    // [                     row 3 to 9 are invalid                     ]
    CHECK_EQ(blob_top_->cpu_data()[0], 32);
    CHECK_EQ(blob_top_->cpu_data()[1], 7);
    CHECK_EQ(blob_top_->cpu_data()[2], 26);
    CHECK_EQ(blob_top_->cpu_data()[3], 28);
    CHECK_EQ(blob_top_->cpu_data()[4], 33);
    CHECK_EQ(blob_top_->cpu_data()[5], 22);
    CHECK_EQ(blob_top_->cpu_data()[6], 36);
    CHECK_EQ(blob_top_->cpu_data()[7], 34);
    CHECK_EQ(blob_top_->cpu_data()[8], 13);
    CHECK_EQ(blob_top_->cpu_data()[9], 32);
    CHECK_EQ(blob_top_->cpu_data()[10], 26);
    CHECK_EQ(blob_top_->cpu_data()[11], 36);
    CHECK_EQ(blob_top_->cpu_data()[12], 34);
    CHECK_EQ(blob_top_->cpu_data()[13], 32);
    CHECK_EQ(blob_top_->cpu_data()[14], 37);
    CHECK_EQ(blob_top_->cpu_data()[15], 21);
    CHECK_EQ(blob_top_->cpu_data()[16], 34);
    CHECK_EQ(blob_top_->cpu_data()[17], 38);
    CHECK_EQ(blob_top_->cpu_data()[18], 18);
    CHECK_EQ(blob_top_->cpu_data()[19], 40);
    CHECK_EQ(blob_top_->cpu_data()[20], 40);
    CHECK_EQ(blob_top_->cpu_data()[21], 22);
    CHECK_EQ(blob_top_->cpu_data()[22], 38);
    CHECK_EQ(blob_top_->cpu_data()[23], 38);
    CHECK_EQ(blob_top_->cpu_data()[24], 40);
    CHECK_EQ(blob_top_->cpu_data()[25], 38);
    CHECK_EQ(blob_top_->cpu_data()[26], 29);
    CHECK_EQ(blob_top_->cpu_data()[27], 29);
    CHECK_EQ(blob_top_->cpu_data()[28], 22);
    CHECK_EQ(blob_top_->cpu_data()[29], 29);
    CHECK_EQ(blob_top_->cpu_data()[30], 29);
    CHECK_EQ(blob_top_->cpu_data()[31], 22);
    CHECK_EQ(blob_top_->cpu_data()[32], 29);
    CHECK_EQ(blob_top_->cpu_data()[33], 29);
    CHECK_EQ(blob_top_->cpu_data()[34], 22);
    CHECK_EQ(blob_top_->cpu_data()[35], 29);
    CHECK_EQ(blob_top_->cpu_data()[36], 22);
    CHECK_EQ(blob_top_->cpu_data()[37], 29);
    CHECK_EQ(blob_top_->cpu_data()[38], 22);
  }
};

typedef ::testing::Types<float, double> Dtypes;
TYPED_TEST_CASE(SPPDetectorLayerTest, Dtypes);

TYPED_TEST(SPPDetectorLayerTest, TestCPUForward) {
  Caffe::set_mode(Caffe::CPU);
  this->TestForward();
}

TYPED_TEST(SPPDetectorLayerTest, TestGPUForward) {
  Caffe::set_mode(Caffe::GPU);
  this->TestForward();
}

}  // namespace caffe
